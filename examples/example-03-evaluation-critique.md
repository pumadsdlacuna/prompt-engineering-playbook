# Example 03 – Evaluation & Critique Prompt

## EN
This prompt is designed to evaluate outputs generated by another model or by a human.
It treats the LLM as a reviewer, not a creator.

## ES
Este prompt está diseñado para evaluar resultados generados por otro modelo o por una persona.
Trata al LLM como revisor, no como creador.

---

## Use Case / Caso de Uso

**EN**  
Evaluate the quality of a proposal, report, model, or prompt before it is used in production.

**ES**  
Evaluar la calidad de una propuesta, reporte, modelo o prompt antes de usarlo en producción.

---

## Prompt Structure

### Role
"You are a critical reviewer with expertise in the relevant domain."

### Context
"The following output is intended for business decision-making."

### Task
"Evaluate the provided content against clear quality criteria."

### Evaluation Criteria
- Clarity
- Logical consistency
- Assumptions
- Risks
- Actionability

### Constraints
- Do not rewrite the content
- Do not add new ideas
- Focus only on evaluation

### Output Format
- Strengths
- Weaknesses
- Risks
- Improvement suggestions (optional)

---

## Why this works / Por qué funciona

**EN**
- Enables quality control
- Supports human-in-the-loop workflows
- Reduces blind trust in AI outputs

**ES**
- Permite control de calidad
- Soporta flujos human-in-the-loop
- Reduce la confianza ciega en outputs de IA
